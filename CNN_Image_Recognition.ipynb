import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation
from keras.constraints import maxnorm
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.utils import np_utils
from keras.datasets import cifar10
import matplotlib.pyplot as plt
from sklearn.metrics import multilabel_confusion_matrix
import seaborn as sns

#Loading the data from cifar10 twice, one to be used for training, and another for testing
(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

#Images are 32x32 pixels . Image data neeeds to be normalised to between 0 and 1
#To do this we divide by total number of pixels 32x32 = 256
#so a range between 0 and 255, choose 255 as it is the max
train_images = train_images.astype('float32')
train_images = train_images / 255.0
test_images = test_images.astype('float32')
test_images = test_images / 255.0

#Labels encoded using one hot encoding ranging from 0-9 (10 classes)
train_labels = np_utils.to_categorical(train_labels)
test_labels = np_utils.to_categorical(test_labels)
#Specify how many classes we need left at the final layer of the neural network 
classes_sum = test_labels.shape[1]

# Creating a sequential frame format
frame = Sequential()

#Stacking layers in frame
#Adding 32 filters of size 3x3 on the images, then using a ReLU activation funtion on layer
frame.add(Conv2D(32, (3, 3), input_shape=train_images.shape[1:], padding='same'))
frame.add(Activation('relu'))
#Omits 0.2 (or 20%) of all connections to account for overfitting
frame.add(Dropout(0.2))
frame.add(BatchNormalization())

#Repeating above but with added filters
frame.add(Conv2D(64, (3, 3), padding='same'))
frame.add(Activation('relu'))
#Adds pooling layer between layers, Allows Neural network to not rely on location of features as much
frame.add(MaxPooling2D(pool_size=(2, 2)))
frame.add(Dropout(0.2))
frame.add(BatchNormalization())

#Repeating above but with added filters
frame.add(Conv2D(64, (3, 3), padding='same'))
frame.add(Activation('relu'))
#Adds pooling layer between layers, Allows Neural network to not rely on location of features as much
frame.add(MaxPooling2D(pool_size=(2, 2)))
frame.add(Dropout(0.2))
frame.add(BatchNormalization())

#Repeats again with more filters
frame.add(Conv2D(128, (3, 3), padding='same'))
frame.add(Activation('relu'))
frame.add(Dropout(0.2))
frame.add(BatchNormalization())

#flattens data
frame.add(Flatten())
frame.add(Dropout(0.2))

#Creating densely connected layers
frame.add(Dense(256, kernel_constraint=maxnorm(3)))
frame.add(Activation('relu'))
frame.add(Dropout(0.2))
frame.add(BatchNormalization())
frame.add(Dense(128, kernel_constraint=maxnorm(3)))
frame.add(Activation('relu'))
frame.add(Dropout(0.2))
frame.add(BatchNormalization())
#Creating a final densely connected layer with as many neurons as there are classes
frame.add(Dense(classes_sum))
#Softmax activation chooses neuron with highest probabalility
frame.add(Activation('softmax'))

#Measures performance of a classification model with an output between 0 and 1, being loss and accuracy
#Uses Adam optimiser algorithm to measure this
frame.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

#Determines the number of times the model is going to be trained
epochs=25

#Outputs the summary of all layers
print(frame.summary())

#Here we train the model an amount set in 'epochs'
#We then attempt to verify it with the testing set to see how accurate the model is
#We store this in 'analytics' so we can see the analytics of the model later
Analytics=frame.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs=epochs, batch_size=64)

#Outputs the models accuracy on testing set
Accuracy = frame.evaluate(test_images, test_labels, verbose=0)
print("Neural Network Accuracy: %.2f%%" % (Accuracy[1]*100))

#Using the 'history' callback function to get the accuracy and loss from training and testing sets every epoch and plots it
# on a graph to visualise how the model is changing each time it is run
Training_Accuracy = Analytics.history['accuracy']
Testing_Accuracy = Analytics.history['val_accuracy']

Training_Loss = Analytics.history['loss']
Testing_loss = Analytics.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, Training_Accuracy, label='Training Accuracy')
plt.plot(epochs_range, Testing_Accuracy, label='Testing Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Testing Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, Training_Loss, label='Training Loss')
plt.plot(epochs_range, Testing_loss, label='Testing Loss')
plt.legend(loc='upper right')
plt.title('Training and Testing Loss')
plt.show()

#Stores all the predicted labels and all the actual labels from the testing set
pred = frame.predict(test_images) 
pred = np.argmax(pred, axis = 1)[:] 
label = np.argmax(test_labels,axis = 1)[:] 

#Arrays storing the names of all the labels from CIFAR10 and another array to be used as labels in a confusion matrix
label_names=['Airplanes', 'Automobiles', 'Birds', 'Cats', 'Deer', 'Dogs', 'Frogs', 'Horses', 'Ships', 'Trucks']
axis_names=['Negative', 'Positive']

#Creates confusion matrices for each label in the CIFAR10 dataset
#Quick view, For visualization, Look below
mlcm=multilabel_confusion_matrix(label, pred)
print(mlcm[:])
#Shown as
# TN FP
# FN TP

"""for i in range (len(label_names)):
    print(label_names[i],':')
    sns.heatmap(mlcm[i], xticklabels=axis_names, yticklabels=axis_names, annot=True)
    print()
"""
#Commented as using this is instead of doing each heatmap seperately will cause all heatmaps to overlap each other making
# them unreadable

#Outputs heatmap of confusion matrix for each label in CIFAR10 for better visualization
print(label_names[0],':')
sns.heatmap(mlcm[0], xticklabels=axis_names, yticklabels=axis_names, fmt="d", annot=True)

print(label_names[1],':')
sns.heatmap(mlcm[1], xticklabels=axis_names, yticklabels=axis_names, fmt="d", annot=True)

print(label_names[2],':')
sns.heatmap(mlcm[2], xticklabels=axis_names, yticklabels=axis_names, fmt="d", annot=True)

print(label_names[3],':')
sns.heatmap(mlcm[3], xticklabels=axis_names, yticklabels=axis_names, fmt="d", annot=True)

print(label_names[4],':')
sns.heatmap(mlcm[4], xticklabels=axis_names, yticklabels=axis_names, fmt="d", annot=True)

print(label_names[5],':')
sns.heatmap(mlcm[5], xticklabels=axis_names, yticklabels=axis_names, fmt="d", annot=True)

print(label_names[6],':')
sns.heatmap(mlcm[6], xticklabels=axis_names, yticklabels=axis_names, fmt="d", annot=True)

print(label_names[7],':')
sns.heatmap(mlcm[7], xticklabels=axis_names, yticklabels=axis_names, fmt="d", annot=True)

print(label_names[8],':')
sns.heatmap(mlcm[8], xticklabels=axis_names, yticklabels=axis_names, fmt="d", annot=True)

print(label_names[9],':')
sns.heatmap(mlcm[9], xticklabels=axis_names, yticklabels=axis_names, fmt="d", annot=True)
